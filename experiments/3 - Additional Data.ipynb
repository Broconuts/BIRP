{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Experiment 3: Additional Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from os import chdir, getcwd\n",
                "\n",
                "if not getcwd().lower().endswith(\"gb-birp\"):\n",
                "    chdir(\"..\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "run_id = \"additional_data\"\n",
                "batch_size = 64\n",
                "epochs = 50\n",
                "initial_lr = 1e-3\n",
                "end_lr = 1e-5"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "import pickle\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import tensorflow as tf\n",
                "%load_ext tensorboard\n",
                "from tensorflow.keras import Model\n",
                "from tensorflow.keras.layers import LSTM, Dropout, Dense, Concatenate, Conv1D, Flatten, Conv2D\n",
                "import src.data.utils as data_utils\n",
                "import src.prediction.eval_tools as eval_tools\n",
                "\n",
                "\n",
                "tf.random.set_seed(17)\n",
                "\n",
                "print(\"Available GPUs: \", len(tf.config.list_physical_devices('GPU')))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_windowified_dataset(scale_weights: bool) -> tuple:\n",
                "    # Generate basic datasets.\n",
                "    train_dates = pd.date_range(\"01/01/2016\", \"31/12/2018\")\n",
                "    test_dates = pd.date_range(\"01/01/2019\", \"31/12/2019\")\n",
                "    train_grid = get_dataset(train_dates)\n",
                "    test_grid = get_dataset(test_dates)\n",
                "\n",
                "    norm_train_grid, norm_test_grid = normalize_dataset(train_grid, test_grid)\n",
                "\n",
                "    train_inputs, train_labels = data_utils.generate_data_windows(\n",
                "        norm_train_grid, train_grid, input_timesteps=7)\n",
                "    test_inputs, test_labels = data_utils.generate_data_windows(\n",
                "        norm_test_grid, test_grid, input_timesteps=7)\n",
                "\n",
                "    # One-Hot Encode Labels.\n",
                "    train_labels = one_hot_encode_labels(train_labels)\n",
                "    test_labels = one_hot_encode_labels(test_labels)\n",
                "\n",
                "    # Get sample weights.\n",
                "    sample_weights = data_utils.calculate_sample_weights(data=(train_inputs,\n",
                "                                                               train_labels),\n",
                "                                                         scale=scale_weights)\n",
                "\n",
                "    return train_inputs, train_labels, test_inputs, test_labels, sample_weights\n",
                "\n",
                "\n",
                "def get_dataset(dates: pd.DatetimeIndex) -> tuple:\n",
                "    data = data_utils.get_dataset(\n",
                "        date_range=dates,\n",
                "        auxiliary_data=[\"weather\", \"events\"],\n",
                "        encode_event_data=True,\n",
                "    )\n",
                "    return data\n",
                "\n",
                "\n",
                "def one_hot_encode_labels(raw_labels: np.ndarray) -> np.ndarray:\n",
                "    new_labels = np.empty([len(raw_labels), 2], dtype=np.int8)\n",
                "    for i, label in enumerate(raw_labels):\n",
                "        if label == 0:\n",
                "            new_labels[i] = np.asarray([1, 0], dtype=np.int8)\n",
                "        else:\n",
                "            new_labels[i] = np.asarray([0, 1], dtype=np.int8)\n",
                "    return new_labels\n",
                "\n",
                "\n",
                "def normalize_dataset(train_grid: pd.DataFrame,\n",
                "                      test_grid: pd.DataFrame) -> tuple:\n",
                "    # Normalize breakin values. We normalize on the training data maximum.\n",
                "    maximum_breakins = data_utils.determine_global_max(train_grid)\n",
                "    norm_train_grid = data_utils.scale_breakin_values(\n",
                "        train_grid.copy(deep=True), maximum_breakins)\n",
                "    norm_test_grid = data_utils.scale_breakin_values(test_grid.copy(deep=True),\n",
                "                                                     maximum_breakins)\n",
                "\n",
                "    # Normalize weather data.\n",
                "    norm_train_grid, norm_test_grid = data_utils.scale_weather_values(\n",
                "        norm_train_grid, norm_test_grid)\n",
                "\n",
                "    return norm_train_grid, norm_test_grid"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "input_train, labels_train, input_test, labels_test, sample_weights = get_windowified_dataset(\n",
                "    scale_weights=True)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_through_training_pipeline(\n",
                "    log_base_directory: str,\n",
                "    run_id: int,\n",
                "    model: Model,\n",
                "    input_train: np.ndarray,\n",
                "    labels_train: np.ndarray,\n",
                "    input_test: np.ndarray,\n",
                "    labels_test: np.ndarray,\n",
                "    sample_weights: np.ndarray,\n",
                "    batch_size: int,\n",
                "    epochs: int,\n",
                "    initial_lr: float,\n",
                "    end_lr: float,\n",
                "):\n",
                "    log_dir = f\"logs/binary_classification/{log_base_directory}/run_{run_id}\"\n",
                "\n",
                "    decay_steps = math.floor(input_train[0].shape[0] / batch_size) * epochs\n",
                "\n",
                "    lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
                "        initial_learning_rate=initial_lr,\n",
                "        end_learning_rate=end_lr,\n",
                "        decay_steps=decay_steps,\n",
                "    )\n",
                "\n",
                "    model.compile(\n",
                "        # loss=tf.keras.losses.BinaryFocalCrossentropy(),\n",
                "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
                "        optimizer=tf.optimizers.Adam(learning_rate=lr_schedule),\n",
                "        metrics=[\"accuracy\"],\n",
                "    )\n",
                "\n",
                "    callbacks = [\n",
                "        tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
                "                                         patience=10,\n",
                "                                         mode='min'),\n",
                "        tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
                "    ]\n",
                "\n",
                "    model.fit(x=input_train,\n",
                "              y=labels_train,\n",
                "              shuffle=True,\n",
                "              batch_size=batch_size,\n",
                "              validation_data=(input_test, labels_test),\n",
                "              epochs=epochs,\n",
                "              callbacks=callbacks)\n",
                "\n",
                "    predictions_test = model.predict(input_test)\n",
                "    eval_tools.calculate_metrics(predictions_test, labels_test)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Base_Dense_Classifier(Model):\n",
                "    \"\"\"\n",
                "    A Single-Timestep Dense classifier classifier.\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self):\n",
                "        super(Base_Dense_Classifier, self).__init__()\n",
                "        self.input_breakins = Dense(units=25)\n",
                "        self.input_date = Dense(units=2)\n",
                "        self.input_weather = Dense(units=11)\n",
                "        self.input_events = Dense(units=9)\n",
                "        self.input_target_cell = Dense(units=25)\n",
                "        self.concat = Concatenate()\n",
                "        self.hidden_layer_1 = Dense(units=47, activation=\"ReLU\")\n",
                "        self.dropout_layer_1 = Dropout(rate=0.8)\n",
                "        self.hidden_layer_2 = Dense(units=25, activation=\"ReLU\")\n",
                "        self.dropout_layer_2 = Dropout(rate=0.5)\n",
                "        self.hidden_layer_3 = Dense(units=10, activation=\"ReLU\")\n",
                "        self.output_layer = Dense(units=2, activation=\"softmax\")\n",
                "\n",
                "    def call(self, inputs):\n",
                "        \"\"\"\n",
                "        Runs input data through the Neural Network.\n",
                "        \"\"\"\n",
                "        input_breakins = self.input_breakins(inputs[0])\n",
                "        input_date = self.input_date(inputs[1])\n",
                "        input_weather = self.input_weather(inputs[2])\n",
                "        input_events = self.input_events(inputs[3])\n",
                "        input_targets = self.input_target_cell(inputs[4])\n",
                "        data = self.concat([\n",
                "            input_breakins, input_date, input_weather, input_events,\n",
                "            input_targets\n",
                "        ])\n",
                "        data = self.hidden_layer_1(data)\n",
                "        data = self.dropout_layer_1(data)\n",
                "        data = self.hidden_layer_2(data)\n",
                "        data = self.dropout_layer_2(data)\n",
                "        data = self.hidden_layer_3(data)\n",
                "        return self.output_layer(data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def transform_to_single_step(inputs: list) -> np.ndarray:\n",
                "    \"\"\"\n",
                "    Takes a regular dataset (windowified for multiple input timesteps) and instead turns it into\n",
                "    single timestep inputs.\n",
                "    \"\"\"\n",
                "    single_timestep_inputs = []\n",
                "    # Iterate over inputs, which is a list of ndarrays.\n",
                "    for input_array in inputs:\n",
                "        # Skip target cell input array because it has no time window dimension.\n",
                "        if len(input_array.shape) == 2:\n",
                "            single_timestep_inputs.append(input_array)\n",
                "            continue\n",
                "        # Only use last timestep of each data window and reduce dimensionality by 1.\n",
                "        reduced_input = input_array[:, -1, :].reshape(\n",
                "            [input_array.shape[0], input_array.shape[-1]])\n",
                "        single_timestep_inputs.append(reduced_input)\n",
                "    return single_timestep_inputs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "run_through_training_pipeline(\n",
                "    log_base_directory=\"basic_dense\",\n",
                "    model=Base_Dense_Classifier(),\n",
                "    run_id=run_id,\n",
                "    input_train=transform_to_single_step(input_train),\n",
                "    labels_train=labels_train,\n",
                "    input_test=transform_to_single_step(input_test),\n",
                "    labels_test=labels_test,\n",
                "    sample_weights=sample_weights,\n",
                "    batch_size=batch_size,\n",
                "    epochs=epochs,\n",
                "    initial_lr=initial_lr,\n",
                "    end_lr=end_lr)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow import expand_dims\n",
                "\n",
                "\n",
                "class Multi_Input_Dense_Classifier(Model):\n",
                "    \"\"\"\n",
                "    Takes multiple timesteps in the same dense input layer and tries to predict whether there will\n",
                "    be a break-in on the next day.\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self):\n",
                "        super(Multi_Input_Dense_Classifier, self).__init__()\n",
                "        self.input_breakins_day_0 = Dense(units=25)\n",
                "        self.input_breakins_day_1 = Dense(units=25)\n",
                "        self.input_breakins_day_2 = Dense(units=25)\n",
                "        self.input_breakins_day_3 = Dense(units=25)\n",
                "        self.input_breakins_day_4 = Dense(units=25)\n",
                "        self.input_breakins_day_5 = Dense(units=25)\n",
                "        self.input_breakins_day_6 = Dense(units=25)\n",
                "\n",
                "        self.input_date = Dense(units=2)\n",
                "        self.input_weather = Dense(units=11)\n",
                "        self.input_events = Dense(units=9)\n",
                "        self.input_target_cell = Dense(units=25)\n",
                "        self.concatenate = Concatenate()\n",
                "\n",
                "        self.hidden_layer_1 = Dense(units=221, activation=\"relu\")\n",
                "        self.dropout_layer_1 = Dropout(rate=0.8)\n",
                "        self.hidden_layer_2 = Dense(units=50, activation=\"relu\")\n",
                "        self.dropout_layer_2 = Dropout(rate=0.5)\n",
                "        self.hidden_layer_3 = Dense(units=27, activation=\"relu\")\n",
                "        self.output_layer = Dense(units=2, activation=\"softmax\")\n",
                "\n",
                "    def call(self, inputs):\n",
                "        x0 = self.input_breakins_day_0(inputs[0][:, 0])\n",
                "        x1 = self.input_breakins_day_1(inputs[0][:, 1])\n",
                "        x2 = self.input_breakins_day_2(inputs[0][:, 2])\n",
                "        x3 = self.input_breakins_day_3(inputs[0][:, 3])\n",
                "        x4 = self.input_breakins_day_4(inputs[0][:, 4])\n",
                "        x5 = self.input_breakins_day_5(inputs[0][:, 5])\n",
                "        x6 = self.input_breakins_day_6(inputs[0][:, 6])\n",
                "        input_date = self.input_date(inputs[1])\n",
                "        input_weather = self.input_weather(inputs[2])\n",
                "        input_events = self.input_events(inputs[3])\n",
                "        target = self.input_target_cell(inputs[4])\n",
                "        input_breakins = self.concatenate([x0, x1, x2, x3, x4, x5, x6, target])\n",
                "        data = self.concatenate([\n",
                "            input_breakins,\n",
                "            input_date,\n",
                "            input_weather,\n",
                "            input_events,\n",
                "        ])\n",
                "        x = self.hidden_layer_1(data)\n",
                "        x = self.dropout_layer_1(x)\n",
                "        x = self.hidden_layer_2(x)\n",
                "        x = self.dropout_layer_2(x)\n",
                "        x = self.hidden_layer_3(x)\n",
                "        return self.output_layer(x)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "run_through_training_pipeline(log_base_directory=\"multi_input_dense\",\n",
                "                              run_id=run_id,\n",
                "                              model=Multi_Input_Dense_Classifier(),\n",
                "                              input_train=input_train,\n",
                "                              labels_train=labels_train,\n",
                "                              input_test=input_test,\n",
                "                              labels_test=labels_test,\n",
                "                              sample_weights=sample_weights,\n",
                "                              batch_size=batch_size,\n",
                "                              epochs=epochs,\n",
                "                              initial_lr=initial_lr,\n",
                "                              end_lr=end_lr)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test1 = tf.random.normal([1, 25])\n",
                "test2 = tf.random.normal([1, 25])\n",
                "test3 = tf.stack([test1, test2], axis=1)\n",
                "test3.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Conv1D_Classifier(Model):\n",
                "    \"\"\"\n",
                "    Takes multiple timesteps in a convolutional layer and tries to predict whether there will be a\n",
                "    break-in on the next day.\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self):\n",
                "        super(Conv1D_Classifier, self).__init__()\n",
                "        self.input_crimes = Conv1D(strides=25,\n",
                "                                   filters=50,\n",
                "                                   activation=\"relu\",\n",
                "                                   kernel_size=7)\n",
                "        self.crime_flatten = Flatten()\n",
                "\n",
                "        self.input_date = Dense(units=2)\n",
                "        self.input_weather = Dense(units=11)\n",
                "        self.input_events = Dense(units=9)\n",
                "        self.input_target_cell = Dense(units=25)\n",
                "        self.concatenate = Concatenate()\n",
                "\n",
                "        self.hidden_layer_1 = Dense(units=100, activation=\"relu\")\n",
                "        self.dropout_layer_1 = Dropout(rate=0.8)\n",
                "        self.hidden_layer_2 = Dense(units=50, activation=\"relu\")\n",
                "        self.dropout_layer_2 = Dropout(rate=0.5)\n",
                "        self.hidden_layer_3 = Dense(units=25, activation=\"relu\")\n",
                "        self.output_layer = Dense(units=2, activation=\"softmax\")\n",
                "\n",
                "    def call(self, inputs):\n",
                "        breakins = self.input_crimes(inputs[0])\n",
                "        breakins = self.crime_flatten(breakins)\n",
                "        date = self.input_date(inputs[1])\n",
                "        weather = self.input_weather(inputs[2])\n",
                "        events = self.input_events(inputs[3])\n",
                "        target_cell = self.input_target_cell(inputs[4])\n",
                "\n",
                "        x = self.concatenate([breakins, date, weather, events, target_cell])\n",
                "        x = self.hidden_layer_1(x)\n",
                "        x = self.dropout_layer_1(x)\n",
                "        x = self.hidden_layer_2(x)\n",
                "        x = self.dropout_layer_2(x)\n",
                "        x = self.hidden_layer_3(x)\n",
                "        return self.output_layer(x)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "run_through_training_pipeline(log_base_directory=\"conv1d\",\n",
                "                              run_id=run_id,\n",
                "                              model=Conv1D_Classifier(),\n",
                "                              input_train=input_train,\n",
                "                              labels_train=labels_train,\n",
                "                              input_test=input_test,\n",
                "                              labels_test=labels_test,\n",
                "                              sample_weights=sample_weights,\n",
                "                              batch_size=batch_size,\n",
                "                              epochs=epochs,\n",
                "                              initial_lr=initial_lr,\n",
                "                              end_lr=end_lr)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Conv2D_Classifier(Model):\n",
                "    \"\"\"\n",
                "    Takes multiple timesteps in a convolutional layer and tries to predict whether there will be a\n",
                "    break-in on the next day.\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self):\n",
                "        super(Conv2D_Classifier, self).__init__()\n",
                "        self.input_crimes = Conv2D(input_shape=(7, 25),\n",
                "                                   data_format=\"channels_first\",\n",
                "                                   strides=1,\n",
                "                                   filters=32,\n",
                "                                   activation=\"relu\",\n",
                "                                   kernel_size=(3, 3),\n",
                "                                   name=\"input_crimes\")\n",
                "        self.crime_flatten = Flatten(name=\"flatten\")\n",
                "\n",
                "        self.input_date = Dense(units=2)\n",
                "        self.input_weather = Dense(units=11)\n",
                "        self.input_events = Dense(units=9)\n",
                "        self.input_target_cell = Dense(units=25)\n",
                "        self.concatenate = Concatenate()\n",
                "\n",
                "        self.hidden_layer_1 = Dense(units=100, activation=\"relu\", name=\"hl1\")\n",
                "        self.dropout_layer_1 = Dropout(rate=0.8, name=\"dropout\")\n",
                "        self.hidden_layer_2 = Dense(units=50, activation=\"relu\", name=\"hl2\")\n",
                "        self.dropout_layer_2 = Dropout(rate=0.5, name=\"dropout2\")\n",
                "        self.hidden_layer_3 = Dense(units=25, activation=\"relu\")\n",
                "        self.output_layer = Dense(units=2, activation=\"softmax\", name=\"output\")\n",
                "\n",
                "    def call(self, inputs):\n",
                "        breakins = self.input_crimes(inputs[0])\n",
                "        breakins = self.crime_flatten(breakins)\n",
                "\n",
                "        date = self.input_date(inputs[1])\n",
                "        weather = self.input_weather(inputs[2])\n",
                "        events = self.input_events(inputs[3])\n",
                "        target_cell = self.input_target_cell(inputs[4])\n",
                "\n",
                "        x = self.concatenate([breakins, date, weather, events, target_cell])\n",
                "        x = self.hidden_layer_1(x)\n",
                "        x = self.dropout_layer_1(x)\n",
                "        x = self.hidden_layer_2(x)\n",
                "        x = self.dropout_layer_2(x)\n",
                "        x = self.hidden_layer_3(x)\n",
                "        return self.output_layer(x)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Add an additional dimension to the data for the channel (only one in this case).\n",
                "# Assumes data_format is channels_last (not the default).\n",
                "train_shape = input_train[0].shape\n",
                "test_shape = input_test[0].shape\n",
                "conv_input_train = (input_train[0].reshape([\n",
                "    train_shape[0], 1, train_shape[1], train_shape[2]\n",
                "]), input_train[1], input_train[2], input_train[3], input_train[4])\n",
                "conv_input_test = (input_test[0].reshape([\n",
                "    test_shape[0], 1, test_shape[1], test_shape[2]\n",
                "]), input_test[1], input_test[2], input_test[3], input_test[4])\n",
                "\n",
                "run_through_training_pipeline(log_base_directory=\"conv2d\",\n",
                "                              run_id=run_id,\n",
                "                              model=Conv2D_Classifier(),\n",
                "                              input_train=conv_input_train,\n",
                "                              labels_train=labels_train,\n",
                "                              input_test=conv_input_test,\n",
                "                              labels_test=labels_test,\n",
                "                              sample_weights=sample_weights,\n",
                "                              batch_size=batch_size,\n",
                "                              epochs=epochs,\n",
                "                              initial_lr=initial_lr,\n",
                "                              end_lr=end_lr)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.9.13 ('venv': venv)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.13"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "8b338cff89ed10554a737b69f21de4f998406b17a58d964c85c36e2dedd2cfa0"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
