{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 5: Two-Step Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import chdir, getcwd\n",
    "\n",
    "if not getcwd().lower().endswith(\"gb-birp\"):\n",
    "    chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = 1\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "INITIAL_LR = 1e-3\n",
    "END_LR = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "%load_ext tensorboard\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, Concatenate, Conv1D, Flatten, Conv2D\n",
    "import src.data.utils as data_utils\n",
    "import src.prediction.eval_tools as eval_tools\n",
    "\n",
    "\n",
    "tf.random.set_seed(17)\n",
    "\n",
    "print(\"Available GPUs: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windowified_dataset(scale_weights: bool) -> tuple:\n",
    "    # Generate basic datasets.\n",
    "    train_dates = pd.date_range(\"01/01/2016\", \"31/12/2018\")\n",
    "    test_dates = pd.date_range(\"01/01/2019\", \"31/12/2019\")\n",
    "    train_grid = get_dataset(train_dates)\n",
    "    test_grid = get_dataset(test_dates)\n",
    "\n",
    "    norm_train_grid, norm_test_grid = normalize_dataset(train_grid, test_grid)\n",
    "\n",
    "    train_inputs, train_labels = data_utils.generate_data_windows(\n",
    "        norm_train_grid, train_grid, input_timesteps=7)\n",
    "    test_inputs, test_labels = data_utils.generate_data_windows(\n",
    "        norm_test_grid, test_grid, input_timesteps=7)\n",
    "\n",
    "    # One-Hot Encode Labels.\n",
    "    train_labels = one_hot_encode_labels(train_labels)\n",
    "    test_labels = one_hot_encode_labels(test_labels)\n",
    "\n",
    "    # Get sample weights.\n",
    "    sample_weights = data_utils.calculate_sample_weights(data=(train_inputs,\n",
    "                                                               train_labels),\n",
    "                                                         scale=scale_weights)\n",
    "\n",
    "    return train_inputs, train_labels, test_inputs, test_labels, sample_weights\n",
    "\n",
    "\n",
    "def get_dataset(dates: pd.DatetimeIndex) -> tuple:\n",
    "    data = data_utils.get_dataset(\n",
    "        date_range=dates,\n",
    "        auxiliary_data=[\"weather\", \"events\"],\n",
    "        encode_event_data=True,\n",
    "    )\n",
    "    return data\n",
    "\n",
    "\n",
    "def one_hot_encode_labels(raw_labels: np.ndarray) -> np.ndarray:\n",
    "    new_labels = np.empty([len(raw_labels), 2], dtype=np.int8)\n",
    "    for i, label in enumerate(raw_labels):\n",
    "        if label == 0:\n",
    "            new_labels[i] = np.asarray([1, 0], dtype=np.int8)\n",
    "        else:\n",
    "            new_labels[i] = np.asarray([0, 1], dtype=np.int8)\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "def normalize_dataset(train_grid: pd.DataFrame,\n",
    "                      test_grid: pd.DataFrame) -> tuple:\n",
    "    # Normalize breakin values. We normalize on the training data maximum.\n",
    "    maximum_breakins = data_utils.determine_global_max(train_grid)\n",
    "    norm_train_grid = data_utils.scale_breakin_values(\n",
    "        train_grid.copy(deep=True), maximum_breakins)\n",
    "    norm_test_grid = data_utils.scale_breakin_values(test_grid.copy(deep=True),\n",
    "                                                     maximum_breakins)\n",
    "\n",
    "    # Normalize weather data.\n",
    "    norm_train_grid, norm_test_grid = data_utils.scale_weather_values(\n",
    "        norm_train_grid, norm_test_grid)\n",
    "\n",
    "    return norm_train_grid, norm_test_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train, labels_train, input_test, labels_test, sample_weights = get_windowified_dataset(\n",
    "    scale_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import src.prediction.eval_tools as eval_tools\n",
    "\n",
    "\n",
    "def run_through_training_pipeline(\n",
    "    model: tf.keras.Model,\n",
    "    input_train: np.ndarray,\n",
    "    labels_train: np.ndarray,\n",
    "    input_test: np.ndarray,\n",
    "    labels_test: np.ndarray,\n",
    "    sample_weights: np.ndarray = None,\n",
    "    run_id: int = RUN_ID,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    epochs: int = EPOCHS,\n",
    "    initial_lr: float = INITIAL_LR,\n",
    "    end_lr: float = END_LR,\n",
    "    decay_steps: int = None,\n",
    "    shuffle: bool = True,\n",
    "):\n",
    "    log_dir = f\"logs/lstm/run_{run_id}\"\n",
    "\n",
    "    if not decay_steps:\n",
    "        decay_steps = math.floor(input_train[0].shape[0] / batch_size) * epochs\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "        initial_learning_rate=initial_lr,\n",
    "        end_learning_rate=end_lr,\n",
    "        decay_steps=decay_steps,\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        optimizer=tf.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    ]\n",
    "\n",
    "    model.fit(x=input_train,\n",
    "              y=labels_train,\n",
    "              sample_weight=sample_weights,\n",
    "              shuffle=shuffle,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(input_test, labels_test),\n",
    "              epochs=epochs,\n",
    "              callbacks=callbacks)\n",
    "\n",
    "    predictions_test = model.predict(input_test)\n",
    "    eval_tools.calculate_metrics(predictions_test, labels_test)\n",
    "\n",
    "\n",
    "def transform_to_single_step(inputs: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Takes a regular dataset (windowified for multiple input timesteps) and instead turns it into\n",
    "    single timestep inputs.\n",
    "    \"\"\"\n",
    "    single_timestep_inputs = []\n",
    "    # Iterate over inputs, which is a list of ndarrays.\n",
    "    for input_array in inputs:\n",
    "        # Skip target cell input array because it has no time window dimension.\n",
    "        if len(input_array.shape) == 2:\n",
    "            single_timestep_inputs.append(input_array)\n",
    "            continue\n",
    "        # Only use last timestep of each data window and reduce dimensionality by 1.\n",
    "        reduced_input = input_array[:, -1, :].reshape(\n",
    "            [input_array.shape[0], input_array.shape[-1]])\n",
    "        single_timestep_inputs.append(reduced_input)\n",
    "    return single_timestep_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_step_input_train = transform_to_single_step(input_train)\n",
    "single_step_input_test = transform_to_single_step(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialNetwork(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    A simple, single-timestep dense classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SpatialNetwork, self).__init__()\n",
    "        self.input_breakins = tf.keras.layers.Dense(units=25)\n",
    "        # self.input_date = tf.keras.layers.Dense(units=2)\n",
    "        # self.input_weather = tf.keras.layers.Dense(units=11)\n",
    "        # self.input_events = tf.keras.layers.Dense(units=9)\n",
    "        self.input_target_cell = tf.keras.layers.Dense(units=25)\n",
    "        self.concat = tf.keras.layers.Concatenate()\n",
    "        self.hidden_layer_1 = tf.keras.layers.Dense(units=50,\n",
    "                                                    activation=\"ReLU\")\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(rate=0.2)\n",
    "        self.hidden_layer_2 = tf.keras.layers.Dense(units=50,\n",
    "                                                    activation=\"ReLU\")\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(rate=0.2)\n",
    "        self.hidden_layer_3 = tf.keras.layers.Dense(units=50,\n",
    "                                                    activation=\"ReLU\")\n",
    "        self.dropout_3 = tf.keras.layers.Dropout(rate=0.2)\n",
    "        self.hidden_layer_4 = tf.keras.layers.Dense(units=50,\n",
    "                                                    activation=\"ReLU\")\n",
    "        self.dropout_4 = tf.keras.layers.Dropout(rate=0.2)\n",
    "        self.hidden_layer_5 = tf.keras.layers.Dense(units=50,\n",
    "                                                    activation=\"ReLU\")\n",
    "        self.dropout_5 = tf.keras.layers.Dropout(rate=0.2)\n",
    "        self.hidden_layer_6 = tf.keras.layers.Dense(units=25,\n",
    "                                                    activation=\"ReLU\")\n",
    "        self.output_layer = tf.keras.layers.Dense(units=2,\n",
    "                                                  activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.digest_features(inputs)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def digest_features(self, inputs):\n",
    "        input_breakins = self.input_breakins(inputs[0])\n",
    "        # input_date = self.input_date(inputs[1])\n",
    "        # input_weather = self.input_weather(inputs[2])\n",
    "        # input_events = self.input_events(inputs[3])\n",
    "        input_targets = self.input_target_cell(inputs[4])\n",
    "        x = self.concat([\n",
    "            input_breakins,  # input_date, input_weather, input_events,\n",
    "            input_targets\n",
    "        ])\n",
    "        x = self.hidden_layer_1(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.hidden_layer_2(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.hidden_layer_3(x)\n",
    "        x = self.dropout_3(x)\n",
    "        x = self.hidden_layer_4(x)\n",
    "        x = self.dropout_4(x)\n",
    "        x = self.hidden_layer_5(x)\n",
    "        x = self.dropout_5(x)\n",
    "        return self.hidden_layer_6(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_network = SpatialNetwork()\n",
    "run_through_training_pipeline(\n",
    "    model=spatial_network,\n",
    "    epochs=50,\n",
    "    input_train=single_step_input_train,\n",
    "    labels_train=labels_train,\n",
    "    input_test=single_step_input_test,\n",
    "    labels_test=labels_test,\n",
    "    # sample_weights=sample_weights,\n",
    "    shuffle=False,\n",
    "    batch_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(input) -> list:\n",
    "    breakins_unprocessed = input[0]\n",
    "    num_samples = breakins_unprocessed.shape[0]\n",
    "    timesteps = breakins_unprocessed.shape[1]\n",
    "    breakins_processed = np.empty((num_samples, timesteps, 25),\n",
    "                                  dtype=np.float32)\n",
    "    date = input[1]\n",
    "    weather = input[2]\n",
    "    events = input[3]\n",
    "    target_cell = input[4]\n",
    "    for i in range(num_samples):\n",
    "        breakins = breakins_unprocessed[i]\n",
    "        target_cell_temp = np.array([target_cell[i]] * timesteps)\n",
    "        breakins_processed[i] = spatial_network.digest_features(\n",
    "            (breakins, date[i], weather[i], events[i], target_cell_temp))\n",
    "    return (breakins_processed, date, weather, events, target_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test_new = prepare_input(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"test_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump((input_test_new, labels_test), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train_new = prepare_input(input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"train_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump((input_train_new, labels_train, sample_weights), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(input, labels, timesteps_per_input=7):\n",
    "    new_dataset_length = labels.shape[0] - timesteps_per_input\n",
    "    # Decompose input tuple into components.\n",
    "    breakins, dates, weather, events, target_cell = input\n",
    "    breakins_new = np.empty(\n",
    "        (new_dataset_length, timesteps_per_input, breakins.shape[-1]))\n",
    "    dates_new = np.empty(\n",
    "        (new_dataset_length, timesteps_per_input, dates.shape[-1]))\n",
    "    weather_new = np.empty(\n",
    "        (new_dataset_length, timesteps_per_input, weather.shape[-1]))\n",
    "    events_new = np.empty(\n",
    "        (new_dataset_length, timesteps_per_input, events.shape[-1]))\n",
    "    target_cell_new = np.empty(\n",
    "        (new_dataset_length, timesteps_per_input, target_cell.shape[-1]))\n",
    "    labels_new = np.empty((new_dataset_length, labels.shape[1]))\n",
    "    for sample_index in range(new_dataset_length):\n",
    "        for timestep_index in range(timesteps_per_input):\n",
    "            breakins_new[sample_index,\n",
    "                         timestep_index] = breakins[sample_index,\n",
    "                                                    timestep_index]\n",
    "            dates_new[sample_index,\n",
    "                      timestep_index] = dates[sample_index + timestep_index]\n",
    "            weather_new[sample_index, timestep_index] = weather[sample_index +\n",
    "                                                                timestep_index]\n",
    "            events_new[sample_index,\n",
    "                       timestep_index] = events[sample_index + timestep_index]\n",
    "            target_cell_new[sample_index,\n",
    "                            timestep_index] = target_cell[sample_index +\n",
    "                                                          timestep_index]\n",
    "        labels_new[sample_index] = labels[sample_index + timesteps_per_input]\n",
    "    input_new = (breakins_new, dates_new, weather_new, events_new,\n",
    "                 target_cell_new)\n",
    "    digested_input = spatial_network.digest_features(input_new).numpy()\n",
    "    return digested_input, labels_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"train_data.pkl\", \"rb\") as f:\n",
    "    input_train_new, labels_train, sample_weights = pickle.load(f)\n",
    "with open(\"test_data.pkl\", \"rb\") as f:\n",
    "    input_test_new, labels_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalNetwork(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    A classifier feeding deep stuff to an LSTM.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TemporalNetwork, self).__init__()\n",
    "        self.lstm = tf.keras.layers.LSTM(25, return_sequences=True)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.input_date = tf.keras.layers.Dense(units=2)\n",
    "        self.input_weather = Dense(units=11)\n",
    "        self.input_events = Dense(units=9)\n",
    "        self.input_target_cell = tf.keras.layers.Dense(units=25)\n",
    "        self.concatenate = tf.keras.layers.Concatenate()\n",
    "        self.hidden_layer_1 = tf.keras.layers.Dense(units=100,\n",
    "                                                    activation=\"ReLU\")\n",
    "        self.hidden_layer_2 = tf.keras.layers.Dense(units=50,\n",
    "                                                    activation=\"ReLU\")\n",
    "        self.hidden_layer_3 = tf.keras.layers.Dense(units=25,\n",
    "                                                    activation=\"ReLU\")\n",
    "        self.output_layer = tf.keras.layers.Dense(units=2,\n",
    "                                                  activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.lstm(inputs[0])\n",
    "        x = self.flatten(x)\n",
    "        date = self.input_date(inputs[1])\n",
    "        weather = self.input_weather(inputs[2])\n",
    "        events = self.input_events(inputs[3])\n",
    "        target_cell = self.input_target_cell(inputs[4])\n",
    "        x = self.concatenate([x, date, weather, events, target_cell])\n",
    "        x = self.hidden_layer_1(x)\n",
    "        x = self.hidden_layer_2(x)\n",
    "        x = self.hidden_layer_3(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "temporal_network = TemporalNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_through_training_pipeline(\n",
    "    model=temporal_network,\n",
    "    epochs=5,\n",
    "    input_train=input_train_new,\n",
    "    labels_train=labels_train,\n",
    "    input_test=input_test_new,\n",
    "    labels_test=labels_test,\n",
    "    # sample_weights=sample_weights,\n",
    "    decay_steps=100,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalNetwork(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    A classifier feeding deep stuff to an LSTM.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TemporalNetwork, self).__init__()\n",
    "        self.input_day_0 = Dense(units=25)\n",
    "        self.input_day_1 = Dense(units=25)\n",
    "        self.input_day_2 = Dense(units=25)\n",
    "        self.input_day_3 = Dense(units=25)\n",
    "        self.input_day_4 = Dense(units=25)\n",
    "        self.input_day_5 = Dense(units=25)\n",
    "        self.input_day_6 = Dense(units=25)\n",
    "        self.concatenate = tf.keras.layers.Concatenate()\n",
    "        self.input_date = tf.keras.layers.Dense(units=2)\n",
    "        self.input_weather = Dense(units=11)\n",
    "        self.input_events = Dense(units=9)\n",
    "        self.input_target_cell = tf.keras.layers.Dense(units=25)\n",
    "        self.hidden_layer_1 = tf.keras.layers.Dense(units=100,\n",
    "                                                    activation=\"ReLU\")\n",
    "        self.hidden_layer_2 = tf.keras.layers.Dense(units=50,\n",
    "                                                    activation=\"ReLU\")\n",
    "        self.hidden_layer_3 = tf.keras.layers.Dense(units=25,\n",
    "                                                    activation=\"ReLU\")\n",
    "        self.output_layer = tf.keras.layers.Dense(units=2,\n",
    "                                                  activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x0 = self.input_day_0(inputs[0][:, 0])\n",
    "        x1 = self.input_day_1(inputs[0][:, 1])\n",
    "        x2 = self.input_day_2(inputs[0][:, 2])\n",
    "        x3 = self.input_day_3(inputs[0][:, 3])\n",
    "        x4 = self.input_day_4(inputs[0][:, 4])\n",
    "        x5 = self.input_day_5(inputs[0][:, 5])\n",
    "        x6 = self.input_day_6(inputs[0][:, 6])\n",
    "        x = self.concatenate([x0, x1, x2, x3, x4, x5, x6])\n",
    "        date = self.input_date(inputs[1])\n",
    "        weather = self.input_weather(inputs[2])\n",
    "        events = self.input_events(inputs[3])\n",
    "        target_cell = self.input_target_cell(inputs[4])\n",
    "        x = self.concatenate([x, date, weather, events, target_cell])\n",
    "        x = self.hidden_layer_1(x)\n",
    "        x = self.hidden_layer_2(x)\n",
    "        x = self.hidden_layer_3(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "temporal_network = TemporalNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_through_training_pipeline(\n",
    "    model=temporal_network,\n",
    "    epochs=5,\n",
    "    input_train=input_train_new,\n",
    "    labels_train=labels_train,\n",
    "    input_test=input_test_new,\n",
    "    labels_test=labels_test,\n",
    "    # sample_weights=sample_weights,\n",
    "    decay_steps=100,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalNetwork(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    A classifier feeding deep stuff to an LSTM.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TemporalNetwork, self).__init__()\n",
    "        self.convolutional_layer = tf.keras.layers.Conv1D(strides=25,\n",
    "                                                          filters=50,\n",
    "                                                          activation=\"relu\",\n",
    "                                                          kernel_size=7)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.input_date = tf.keras.layers.Dense(units=2)\n",
    "        self.input_weather = Dense(units=11)\n",
    "        self.input_events = Dense(units=9)\n",
    "        self.input_target_cell = tf.keras.layers.Dense(units=25)\n",
    "        self.concatenate = tf.keras.layers.Concatenate()\n",
    "        self.hidden_layer_1 = tf.keras.layers.Dense(units=100,\n",
    "                                                    activation=\"ReLU\")\n",
    "        self.hidden_layer_2 = tf.keras.layers.Dense(units=50,\n",
    "                                                    activation=\"ReLU\")\n",
    "        self.hidden_layer_3 = tf.keras.layers.Dense(units=25,\n",
    "                                                    activation=\"ReLU\")\n",
    "        self.output_layer = tf.keras.layers.Dense(units=2,\n",
    "                                                  activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.convolutional_layer(inputs[0])\n",
    "        x = self.flatten(x)\n",
    "        date = self.input_date(inputs[1])\n",
    "        weather = self.input_weather(inputs[2])\n",
    "        events = self.input_events(inputs[3])\n",
    "        target_cell = self.input_target_cell(inputs[4])\n",
    "        x = self.concatenate([x, date, weather, events, target_cell])\n",
    "        x = self.hidden_layer_1(x)\n",
    "        x = self.hidden_layer_2(x)\n",
    "        x = self.hidden_layer_3(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "temporal_network = TemporalNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_through_training_pipeline(\n",
    "    model=temporal_network,\n",
    "    epochs=5,\n",
    "    input_train=input_train_new,\n",
    "    labels_train=labels_train,\n",
    "    input_test=input_test_new,\n",
    "    labels_test=labels_test,\n",
    "    # sample_weights=sample_weights,\n",
    "    decay_steps=100,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49ed72d46a7a4699a9344b4abaf86a5deb96c716c67cb4ff469b1dc83ef3b550"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
